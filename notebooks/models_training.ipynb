{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60594bba-029f-408c-b470-2c8d61cabe75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T06:55:26.340988Z",
     "iopub.status.busy": "2025-05-19T06:55:26.340715Z",
     "iopub.status.idle": "2025-05-19T06:55:26.349321Z",
     "shell.execute_reply": "2025-05-19T06:55:26.347229Z",
     "shell.execute_reply.started": "2025-05-19T06:55:26.340969Z"
    }
   },
   "source": [
    "# Analyse de sentiments pour Air Paradis\n",
    "**Auteur:** Didier DRACHE  \n",
    "**Date:** Mars-Avril 2025\n",
    "\n",
    "Ce notebook présente le développement d'un système d'analyse de sentiments pour détecter les bad buzz potentiels sur les réseaux sociaux pour la compagnie aérienne Air Paradis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f90fec-a566-460f-9982-156b8094d7be",
   "metadata": {},
   "source": [
    "## 1. Introduction et Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ab1cd-ab5c-47e7-a797-fb32de9b65bf",
   "metadata": {},
   "source": [
    "### 1.1 Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8850c-017f-41df-ae50-a98d066fff0e",
   "metadata": {},
   "source": [
    "Les bibliothèques ci-dessous sont utilisées pour l'analyse de données, la modélisation et le suivi des expériences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d105e0-92e1-4a1f-a373-1204509760fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Introduction et Configuration\n",
    "# Projet: Détection de sentiment pour Air Paradis\n",
    "# Auteur: Didier DRACHE\n",
    "# Date de début: 22 mars 2025\n",
    "\n",
    "# 1.1 Importation des bibliothèques nécessaires\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Ou '2' pour cacher aussi les warnings\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "\n",
    "# Statistiques et métriques\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Prétraitement et modélisation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Embedding, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPooling1D,\n",
    "    Dropout, BatchNormalization, Input, MaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Tracking des expérimentations\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "\n",
    "# Import des fonctions utilitaires personnalisées\n",
    "# Ce fichier contient toutes les fonctions réutilisables\n",
    "from config import CONFIG, update_sample_size, update_lemmatization\n",
    "try:\n",
    "    from utils import *\n",
    "except ImportError:\n",
    "    print(\"Fichier utils.py introuvable, veuillez exécuter d'abord la cellule de création des utilitaires\")\n",
    "    \n",
    "# Configuration de visualisation\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Configuration pour éviter les avertissements\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Forcer l'utilisation du GPU 1 (second GPU)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Vérifier que TensorFlow voit le GPU correctement\n",
    "print(\"TensorFlow GPUs disponibles:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Configurer l'allocation mémoire dynamique pour éviter de saturer la VRAM\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Permettre une allocation mémoire dynamique (libérable)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Allocation mémoire dynamique activée pour tous les GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Erreur lors de la configuration des GPUs:\", e)\n",
    "\n",
    "# Libérer explicitement la mémoire CUDA\n",
    "def clear_gpu_memory():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(\"Mémoire GPU libérée\")\n",
    "\n",
    "download_nltk_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca910df-b939-4970-bc59-ecbf63928e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T17:30:41.434874Z",
     "iopub.status.busy": "2025-04-04T17:30:41.434633Z",
     "iopub.status.idle": "2025-04-04T17:30:41.437209Z",
     "shell.execute_reply": "2025-04-04T17:30:41.436889Z",
     "shell.execute_reply.started": "2025-04-04T17:30:41.434853Z"
    }
   },
   "source": [
    "### 1.2 Création de la structure de répertoires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104eac39-f4e1-4cf5-9d33-314bc0104d21",
   "metadata": {},
   "source": [
    "Configuration de TensorFlow pour utiliser efficacement les GPUs disponibles (RTX 3090) avec allocation mémoire dynamique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e043bc5-572f-4d52-91ed-fb78a942084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Création de la structure de répertoires\n",
    "created_dirs = create_directory_structure()\n",
    "print(f\"Répertoires créés: {created_dirs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8dd61c-ae12-4864-9ee0-42bc0ebe5d84",
   "metadata": {},
   "source": [
    "### 1.3 Configuration de MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1b1bf-b58a-4cd0-8332-3d7c5463ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Configuration de MLflow\n",
    "experiment_id = setup_mlflow_experiment(\"Air Paradis - Analyse de Sentiment\")\n",
    "print(f\"MLflow experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a8792-982f-4eef-8f97-de205af1648a",
   "metadata": {},
   "source": [
    "### 1.4 Informations sur le projet et contexte métier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b754d-84a9-437e-835a-c6a66b72cac2",
   "metadata": {},
   "source": [
    "- **Client:** Air Paradis (compagnie aérienne)\n",
    "- **Objectif:** Développer un modèle de prédiction du sentiment associé aux tweets\n",
    "- **Contexte:** Anticiper les bad buzz sur les réseaux sociaux\n",
    "- **Approches à tester:**\n",
    "  1. Modèle sur mesure simple (classique)\n",
    "  2. Modèle sur mesure avancé (deep learning)\n",
    "  3. Modèle avancé BERT\n",
    "  4. Modèle DistilBERT (alternative légère à BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8bdb0-b9f9-4590-907f-b6680656a986",
   "metadata": {},
   "source": [
    "### 1.5 Informations sur l'environnement d'exécution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ce7e6-f957-4e24-afc2-6035119e741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Informations sur l'environnement d'exécution\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Collecte des informations sur l'environnement\n",
    "import platform, re\n",
    "system_info = f\"**Système:** {platform.system()} {platform.release()}\"\n",
    "python_info = f\"**Python:** {platform.python_version()}\"\n",
    "\n",
    "# Informations GPU\n",
    "gpu_info = []\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        gpu_info.append(f\"GPU disponible: {gpu.name}\")\n",
    "        \n",
    "        # Informations sur la mémoire GPU\n",
    "        try:\n",
    "            import torch\n",
    "            gpu_info.append(f\"Mémoire GPU (CUDA): {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        except:\n",
    "            gpu_info.append(\"Impossible d'obtenir les informations de mémoire GPU via PyTorch\")\n",
    "else:\n",
    "    gpu_info.append(\"Aucun GPU détecté, l'exécution sera plus lente sur CPU\")\n",
    "\n",
    "# Construction du markdown\n",
    "markdown_content = f\"\"\"\n",
    "- {system_info}\n",
    "- {python_info}\n",
    "\"\"\"\n",
    "\n",
    "# Ajouter les informations GPU\n",
    "for info in gpu_info:\n",
    "    info_md =info.replace('/physical_device:','')\n",
    "    info_md = re.sub(r'^([^:]+):', r'**\\1:**', info_md)\n",
    "    markdown_content += f\"- {info_md}\\n\"\n",
    "\n",
    "# Afficher le markdown\n",
    "print_md(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9be070-6100-4ba4-a151-4b4a66a721e6",
   "metadata": {},
   "source": [
    "### 1.6 Paramètres globaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b66baa-c2c6-42ca-b91a-433caa95d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Paramètres globaux\n",
    "\n",
    "# Paramètres globaux pour le projet\n",
    "\n",
    "# taille_echantillon = 100000\n",
    "# taille_echantillon = 10000 # pour debug\n",
    "# taille_echantillon = 20000 # pour debug\n",
    "# taille_echantillon = None # tout le jeu de donnés\n",
    "taille_echantillon = 100000\n",
    "Utiliser_Lemmatisation = False\n",
    "\n",
    "# Mettre à jour la configuration avec la taille d'échantillon\n",
    "CONFIG = update_sample_size(taille_echantillon)\n",
    "CONFIG = update_lemmatization(Utiliser_Lemmatisation)\n",
    "\n",
    "# Fixer les graines aléatoires pour la reproductibilité\n",
    "np.random.seed(CONFIG[\"RANDOM_SEED\"])\n",
    "tf.random.set_seed(CONFIG[\"RANDOM_SEED\"])\n",
    "\n",
    "display(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189ee75-475f-4d65-9fb6-7759f27416bf",
   "metadata": {},
   "source": [
    "- **Graine aléatoire:** 42\n",
    "- **Taille de test:** 20%\n",
    "- **Taille de validation:** 10%\n",
    "- **Vocabulaire maximum:** 50,000 mots\n",
    "- **Taille de batch:** 64\n",
    "- **Époques:** 10\n",
    "- **Longueur max des séquences:** 100\n",
    "- **Dimension des embeddings:** 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd770f2-a068-48ac-9936-0c438821ee6f",
   "metadata": {},
   "source": [
    "## 2. Exploration et Préparation des Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62cac76-f89e-4895-825e-ef80c5451171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T12:17:29.026312Z",
     "iopub.status.busy": "2025-04-15T12:17:29.026084Z",
     "iopub.status.idle": "2025-04-15T12:17:29.030026Z",
     "shell.execute_reply": "2025-04-15T12:17:29.029653Z",
     "shell.execute_reply.started": "2025-04-15T12:17:29.026296Z"
    }
   },
   "source": [
    "Cette section couvre le chargement, l'exploration et la préparation des données Sentiment140 pour l'entraînement des modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158ab7e-957f-4d60-8b5a-ef890fbcec51",
   "metadata": {},
   "source": [
    "### 2.1 Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06625205-0a64-48ee-a234-e0fc40b867dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Exploration et Préparation des Données\n",
    "# 2.1 Chargement des données\n",
    "\n",
    "# Ces modèles sont maintenant définis dans utils.py pour faciliter leur réutilisation\n",
    "from utils import load_sentiment140_data\n",
    "\n",
    "# Chargement des données\n",
    "result = load_sentiment140_data(CONFIG[\"DATA_PATH\"])\n",
    "df = result[0]  # On garde seulement le DataFrame, pas le temps d'exécution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37121d9-3fb6-4a8a-b97a-35e3a9d91cd8",
   "metadata": {},
   "source": [
    "### 2.2 Analyse exploratoire des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985addf-1606-4d0e-ad05-7ab401a2c151",
   "metadata": {},
   "source": [
    "Examinons la structure des données, leur distribution et leurs caractéristiques principales pour mieux comprendre le corpus de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a2d25-364a-4512-a786-4bcf199774c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Analyse exploratoire des données\n",
    "\n",
    "stats_markdown = f\"\"\"\n",
    "#### Statistiques générales du dataset:\n",
    "- **Nombre total d'observations:** {df.shape[0]:,}\n",
    "- **Nombre de variables:** {df.shape[1]}\"\"\"\n",
    "print_md(stats_markdown)\n",
    "\n",
    "# print(\"\\nAperçu des données:\")\n",
    "print_md(f\"\"\"#### Aperçu des données:\"\"\")\n",
    "display(df.head())\n",
    "\n",
    "# print(\"\\nInformations sur les variables:\")\n",
    "print_md(f\"\"\"\\n#### Informations sur les variables:\"\"\")\n",
    "display(df.info())\n",
    "\n",
    "# print(\"\\nStatistiques descriptives:\")\n",
    "print_md(f\"\"\"\\n#### Statistiques descriptives du dataset:\"\"\")\n",
    "display(df.describe(include='all'))\n",
    "\n",
    "# Distribution des sentiments\n",
    "print_md(f\"\"\"\\n#### Distribution des sentiments dans le corpus:\"\"\")\n",
    "stats_markdown = f\"\"\"\n",
    "- **Distribution des sentiments:** \n",
    "  - Négatif: {df[df['sentiment'] == 0].shape[0]:,} tweets ({df[df['sentiment'] == 0].shape[0]/df.shape[0]*100:.1f}%)\n",
    "  - Positif: {df[df['sentiment'] == 1].shape[0]:,} tweets ({df[df['sentiment'] == 1].shape[0]/df.shape[0]*100:.1f}%)\n",
    "\"\"\"\n",
    "print_md(stats_markdown)\n",
    "\n",
    "# print(\"\\nDistribution des sentiments:\")\n",
    "sentiment_counts = df['sentiment'].value_counts().reset_index()\n",
    "sentiment_counts.columns = ['Sentiment', 'Count']\n",
    "sentiment_counts['Sentiment'] = sentiment_counts['Sentiment'].replace({0: 'Négatif', 1: 'Positif'})\n",
    "display(sentiment_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='sentiment', hue='sentiment', data=df, palette=['red', 'green'],legend=False)\n",
    "plt.title('Distribution des sentiments', fontsize=14)\n",
    "plt.xlabel('Sentiment (0 = Négatif, 1 = Positif)')\n",
    "plt.ylabel('Nombre de tweets')\n",
    "plt.xticks([0, 1], ['Négatif', 'Positif'])\n",
    "plt.savefig('visualisations/2_2_sentiment_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb1137f-6df5-4b6f-ae18-404589b94f68",
   "metadata": {},
   "source": [
    "### 2.3 Analyse du texte des tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031d0d2-0a35-458c-94ee-4f9775aaa3f9",
   "metadata": {},
   "source": [
    "Cette section explore la longueur des tweets et leur composition pour identifier des motifs potentiellement utiles pour la modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fdd128-c0b0-47a4-81ae-7eb1d7cde2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Analyse du texte des tweets\n",
    "print(\"\\n=== Analyse du texte des tweets ===\")\n",
    "\n",
    "# Longueur des tweets\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "\n",
    "text_stats_markdown = f\"\"\"\n",
    "#### Caractéristiques des tweets:\n",
    "- **Longueur des tweets:**\n",
    "  - Moyenne: {df['text_length'].mean():.2f} caractères\n",
    "  - Médiane: {df['text_length'].median():.2f} caractères\n",
    "  - Minimum: {df['text_length'].min()} caractères\n",
    "  - Maximum: {df['text_length'].max()} caractères\n",
    "\"\"\"\n",
    "print_md(text_stats_markdown)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='text_length', hue='sentiment', \n",
    "             bins=50, kde=True, palette=['red', 'green'])\n",
    "plt.title('Distribution de la longueur des tweets par sentiment', fontsize=14)\n",
    "plt.xlabel('Longueur du tweet (caractères)')\n",
    "plt.ylabel('Nombre de tweets')\n",
    "plt.savefig('visualisations/2_3_tweet_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Nombre de mots par tweet\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "text_stats_markdown = f\"\"\"\n",
    "#### Nombre de mots:\n",
    "- Moyenne: {df['word_count'].mean():.2f} mots\n",
    "- Médiane: {df['word_count'].median():.2f} mots\n",
    "- Minimum: {df['word_count'].min()} mots\n",
    "- Maximum: {df['word_count'].max()} mots\n",
    "\"\"\"\n",
    "print_md(text_stats_markdown)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='word_count', hue='sentiment', \n",
    "             bins=50, kde=True, palette=['red', 'green'])\n",
    "plt.title('Distribution du nombre de mots par tweet', fontsize=14)\n",
    "plt.xlabel('Nombre de mots')\n",
    "plt.ylabel('Nombre de tweets')\n",
    "plt.savefig('visualisations/2_3_word_count_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c9122-a5f8-41a5-afb9-59e1fec999d2",
   "metadata": {},
   "source": [
    "### 2.4 Analyse des termes les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4b826-146c-4c36-b2fb-028f2ce649bf",
   "metadata": {},
   "source": [
    "Identification des mots qui apparaissent le plus fréquemment dans les tweets positifs et négatifs pour comprendre les différences lexicales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353849fa-39bd-436e-8e32-eac2e39f804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Analyse des termes les plus fréquents\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ces modèles sont maintenant définis dans utils.py pour faciliter leur réutilisation\n",
    "from utils import get_top_words\n",
    "\n",
    "# Télécharger les stopwords si nécessaire\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Créer un markdown pour résumer les termes fréquents\n",
    "terms_markdown = \"#### Termes les plus fréquents:\\n\\n\"\n",
    "terms_markdown += \"| Rang | Terme négatif | Fréquence | Terme positif | Fréquence |\\n\"\n",
    "terms_markdown += \"|------|--------------|-----------|---------------|-----------|\\n\"\n",
    "\n",
    "# Mots les plus fréquents dans les tweets négatifs\n",
    "negative_tweets = df[df['sentiment'] == 0]['text']\n",
    "negative_top_words = get_top_words(negative_tweets)\n",
    "\n",
    "# Mots les plus fréquents dans les tweets positifs\n",
    "positive_tweets = df[df['sentiment'] == 1]['text']\n",
    "positive_top_words = get_top_words(positive_tweets)\n",
    "\n",
    "for i in range(min(20, len(negative_top_words), len(positive_top_words))):\n",
    "    neg_word, neg_count = negative_top_words[i]\n",
    "    pos_word, pos_count = positive_top_words[i]\n",
    "    terms_markdown += f\"| {i+1} | {neg_word} | {neg_count:,} | {pos_word} | {pos_count:,} |\\n\"\n",
    "\n",
    "print_md(terms_markdown)\n",
    "\n",
    "# Visualisation des mots les plus fréquents\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Tweets négatifs\n",
    "plt.subplot(1, 2, 1)\n",
    "words, counts = zip(*negative_top_words)\n",
    "sns.barplot(x=list(counts), y=list(words), hue=list(words), palette='Reds_r',legend=False)\n",
    "plt.title('Top 20 des mots - Tweets Négatifs', fontsize=14)\n",
    "plt.xlabel('Nombre d\\'occurrences')\n",
    "\n",
    "# Tweets positifs\n",
    "plt.subplot(1, 2, 2)\n",
    "words, counts = zip(*positive_top_words)\n",
    "sns.barplot(x=list(counts), y=list(words), hue=list(words), palette='Greens_r',legend=False)\n",
    "plt.title('Top 20 des mots - Tweets Positifs', fontsize=14)\n",
    "plt.xlabel('Nombre d\\'occurrences')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/2_4_top_words_by_sentiment.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fcc00-9d67-45c3-aad1-92484d421498",
   "metadata": {},
   "source": [
    "### 2.5 Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e85411-6eb2-4158-a4a3-9e45cde29416",
   "metadata": {},
   "source": [
    "Nettoyage des textes et préparation des ensembles d'entraînement et de test pour la modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba85cd-933d-4165-81f4-9d5c7de42a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Prétraitement des données\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from utils import clean_text, preprocess_data, save_processed_data, load_processed_data\n",
    "\n",
    "# Prétraitement des données\n",
    "\n",
    "# Utiliser les paramètres de la configuration globale\n",
    "result = preprocess_data(df, \n",
    "                        sample_size=CONFIG[\"SAMPLE_SIZE\"], \n",
    "                        lemmatize=CONFIG[\"USE_LEMMATIZATION\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = result[0]  # On garde seulement les données, pas le temps d'exécution\n",
    "\n",
    "save_processed_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print_md(f\"\"\"\n",
    "#### Résultats du prétraitement:\n",
    "- **Textes nettoyés** avec suppression des URLs, mentions, et caractères spéciaux\n",
    "- **Lemmatisation: {\"Appliquée\" if CONFIG[\"USE_LEMMATIZATION\"] else \"Non appliquée\"}**\n",
    "- **Division des données:**\n",
    "  - Ensemble d'entraînement: {len(X_train):,} exemples\n",
    "  - Ensemble de test: {len(X_test):,} exemples\n",
    "- **Distribution préservée** dans les ensembles d'entraînement et de test\n",
    "- **Temps de prétraitement:** {result[1]:.2f} secondes\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104de88-84e9-45de-8005-d6e16430280e",
   "metadata": {},
   "source": [
    "## 3. Modèles Classiques (\"Modèle sur mesure simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614f8aa-3a03-4e22-9fae-a487d2ac8b91",
   "metadata": {},
   "source": [
    "Cette section présente le développement et l'évaluation de modèles traditionnels de machine learning pour la classification de sentiment des tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6fe45d-e827-41f0-9952-a38c0f857b68",
   "metadata": {},
   "source": [
    "### 3.1 Préparation des fonctions pour les modèles classiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85480562-2842-455c-8480-4a63bf9b5cf5",
   "metadata": {},
   "source": [
    "Définition des fonctions d'entraînement, d'évaluation et de visualisation pour les modèles classiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715d4e1-1393-4279-86be-fb1d77c7ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modèles Classiques (\"Modèle sur mesure simple\")\n",
    "\n",
    "# 3.1 Préparation des fonctions pour les modèles classiques\n",
    "\n",
    "from utils import train_evaluate_classical_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22354bfb-fa7a-4711-9131-be80548dc747",
   "metadata": {},
   "source": [
    "### 3.2 Entraînement et évaluation des modèles classiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41c292-2207-45c7-9f21-8321d8a1d17a",
   "metadata": {},
   "source": [
    "Implémentation et comparaison de plusieurs algorithmes de classification: Régression Logistique, Naive Bayes, SVM et Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2958c-ef27-43d3-809e-05d0d5a4c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Entraînement et évaluation des modèles classiques\n",
    "\n",
    "from utils import train_all_classical_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16973159-8a0b-4ddf-a857-b33dd616044b",
   "metadata": {},
   "source": [
    "### 3.3 Exécution et visualisation des modèles classiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10e3ee-eb17-4d95-a8cf-d3545a7ff2fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:15:54.475280Z",
     "iopub.status.busy": "2025-04-15T13:15:54.475047Z",
     "iopub.status.idle": "2025-04-15T13:15:54.477967Z",
     "shell.execute_reply": "2025-04-15T13:15:54.477480Z",
     "shell.execute_reply.started": "2025-04-15T13:15:54.475265Z"
    }
   },
   "source": [
    "Entraînement des modèles et visualisation comparative de leurs performances et temps d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6365d-8dfe-4dbd-8f06-52be138f6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Exécution et visualisation des modèles classiques\n",
    "\n",
    "from utils import train_evaluate_classical_model, train_all_classical_models, print_md\n",
    "\n",
    "# from utils import train_evaluate_classical_model, train_all_classical_models, print_md\n",
    "# from visualization_utils import plot_model_comparison, plot_model_time_comparison\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# Charger les données prétraitées\n",
    "X_train, X_test, y_train, y_test = load_processed_data()\n",
    "\n",
    "# Entraîner tous les modèles classiques\n",
    "print_md(\"#### ====== Entraînement des modèles classiques ======\")\n",
    "\n",
    "classical_results = train_all_classical_models(X_train, X_test, y_train, y_test)\n",
    "print(f\"Forme du DataFrame classical_results: {classical_results.shape}\")\n",
    "\n",
    "# Afficher les résultats\n",
    "print_md(f\"\"\"#### ====== Résultats des modèles classiques ======\"\"\")\n",
    "\n",
    "display(classical_results)\n",
    "\n",
    "print_md(f\"\"\"\n",
    "#### ====== Comparaison des performances des modèles classiques ======\n",
    "\n",
    "| Modèle | Accuracy | F1 Score | Temps (s) | Taille (MB) |\n",
    "|--------|----------|----------|-----------|-------------|\n",
    "| {classical_results.iloc[0]['Modèle']} | {classical_results.iloc[0]['Accuracy']:.4f} | {classical_results.iloc[0]['F1 Score']:.4f} | {classical_results.iloc[0]['Temps (s)']:.2f} | {classical_results.iloc[0]['Taille (MB)']:.2f} |\n",
    "| {classical_results.iloc[1]['Modèle']} | {classical_results.iloc[1]['Accuracy']:.4f} | {classical_results.iloc[1]['F1 Score']:.4f} | {classical_results.iloc[1]['Temps (s)']:.2f} | {classical_results.iloc[1]['Taille (MB)']:.2f} |\n",
    "| {classical_results.iloc[2]['Modèle']} | {classical_results.iloc[2]['Accuracy']:.4f} | {classical_results.iloc[2]['F1 Score']:.4f} | {classical_results.iloc[2]['Temps (s)']:.2f} | {classical_results.iloc[2]['Taille (MB)']:.2f} |\n",
    "| {classical_results.iloc[3]['Modèle']} | {classical_results.iloc[3]['Accuracy']:.4f} | {classical_results.iloc[3]['F1 Score']:.4f} | {classical_results.iloc[3]['Temps (s)']:.2f} | {classical_results.iloc[3]['Taille (MB)']:.2f} |\n",
    "\"\"\")\n",
    "\n",
    "# Visualiser les performances des modèles\n",
    "plot_model_comparison(classical_results, \n",
    "                      metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "                      filename='visualisations/3_3_classical_models_comparison.png')\n",
    "plot_model_comparison(classical_results, \n",
    "                      metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Visualiser les temps d'entraînement\n",
    "plot_model_time_comparison(classical_results, \n",
    "                          filename='visualisations/3_3_classical_models_time_comparison.png')\n",
    "plot_model_time_comparison(classical_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c5a51-e633-4293-8416-867164f5c673",
   "metadata": {},
   "source": [
    "### 3.4 Analyse approfondie du meilleur modèle classique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed05d7-e219-475a-88e1-aaa5534b7b0c",
   "metadata": {},
   "source": [
    "Étude détaillée des features importantes et du comportement du modèle le plus performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb7546-334c-41f7-ad6c-9ab0292664ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Analyse approfondie du meilleur modèle classique\n",
    "# Trouver le meilleur modèle selon le F1 Score\n",
    "best_model_idx = classical_results['F1 Score'].idxmax()\n",
    "best_model_name = classical_results.loc[best_model_idx, 'Modèle']\n",
    "print(f\"\\nMeilleur modèle classique: {best_model_name}\")\n",
    "\n",
    "# Charger le meilleur modèle\n",
    "best_model_path = f\"models/classical/{best_model_name.replace(' ', '_').lower()}.pkl\"\n",
    "with open(best_model_path, 'rb') as f:\n",
    "    best_pipeline = pickle.load(f)\n",
    "\n",
    "# Accéder au vectoriseur et au modèle\n",
    "vectorizer = best_pipeline.named_steps['vectorizer']\n",
    "model = best_pipeline.named_steps['model']\n",
    "\n",
    "features_md = f\"\"\"\n",
    "#### Features les plus importantes pour {best_model_name}\n",
    "\n",
    "| Feature | Importance |\n",
    "|---------|------------|\n",
    "\"\"\"\n",
    "\n",
    "# Analyse des features importantes (pour les modèles qui le supportent)\n",
    "if hasattr(model, 'coef_'):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Pour les modèles avec une seule classe de sortie (ex: LogisticRegression avec deux classes)\n",
    "    if len(model.coef_.shape) == 2 and model.coef_.shape[0] == 1:\n",
    "        coefficients = model.coef_[0]\n",
    "    else:\n",
    "        coefficients = model.coef_\n",
    "    \n",
    "    # Créer un DataFrame avec les coefficients\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': np.abs(coefficients)\n",
    "    })\n",
    "    \n",
    "    # Trier par importance décroissante\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "    for i in range(min(10, len(feature_importance))):\n",
    "        row = feature_importance.iloc[i]\n",
    "        feat = row.get('feature')  # Utilisation de get() pour éviter les erreurs\n",
    "        imp = row.get('importance')\n",
    "        features_md += f\"| {feat} | {imp:.4f} |\\n\"\n",
    "    print_md(features_md)\n",
    "    \n",
    "    # Afficher les 20 features les plus importantes\n",
    "    print(\"\\nTop 20 des features les plus importantes:\")\n",
    "    display(feature_importance.head(20))\n",
    "\n",
    "    # Visualiser les 20 features les plus importantes\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "    plt.title(f'Top 20 des features les plus importantes - {best_model_name}', fontsize=14)\n",
    "    plt.xlabel('Importance (valeur absolue du coefficient)')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualisations/3_4_feature_importance_{best_model_name.replace(\" \", \"_\").lower()}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "elif hasattr(model, 'feature_importances_'):\n",
    "    # Pour les modèles comme Random Forest\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    })\n",
    "    \n",
    "    # Trier par importance décroissante\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Afficher les 20 features les plus importantes\n",
    "    print(\"\\nTop 20 des features les plus importantes:\")\n",
    "    display(feature_importance.head(20))\n",
    "\n",
    "    \n",
    "    # Visualiser les 20 features les plus importantes\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "    plt.title(f'Top 20 des features les plus importantes - {best_model_name}', fontsize=14)\n",
    "    plt.xlabel('Importance de la feature')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualisations/3_4_feature_importance_{best_model_name.replace(\" \", \"_\").lower()}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"L'extraction des features importantes n'est pas supportée pour le modèle {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e687281-60bc-4bf0-962c-5b6b65652672",
   "metadata": {},
   "source": [
    "### 3.5 Échantillons de prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be9691-1fd4-459f-8c8b-7dad9bffe7c2",
   "metadata": {},
   "source": [
    "Tests du modèle sur des exemples concrets de tweets pour évaluer qualitativement ses performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad03d2-06a7-4cb4-b5dc-a4338f694af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Échantillons de prédictions\n",
    "# Faire des prédictions sur quelques exemples\n",
    "sample_texts = [\n",
    "    \"I absolutely love this product! It's amazing!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The flight was delayed by 2 hours and no compensation was offered.\",\n",
    "    \"Air Paradis has the best customer service I've experienced.\",\n",
    "    \"Not sure how I feel about this, it's just okay I guess.\"\n",
    "]\n",
    "\n",
    "# Nettoyer les textes\n",
    "sample_cleaned = [clean_text(text) for text in sample_texts]\n",
    "\n",
    "# Faire les prédictions\n",
    "sample_predictions = best_pipeline.predict(sample_cleaned)\n",
    "if hasattr(best_pipeline, 'predict_proba'):\n",
    "    sample_proba = best_pipeline.predict_proba(sample_cleaned)[:, 1]\n",
    "else:\n",
    "    sample_proba = None\n",
    "\n",
    "predictions_md = f\"\"\"\n",
    "#### Exemples de prédictions avec {best_model_name}\n",
    "\n",
    "| Texte | Prédiction | Confiance |\n",
    "|-------|------------|-----------|\n",
    "\"\"\"\n",
    "\n",
    "for i, (text, pred) in enumerate(zip(sample_texts, sample_predictions)):\n",
    "    sentiment = \"Positif\" if pred == 1 else \"Négatif\"\n",
    "    confidence = sample_proba[i] if sample_proba is not None else \"N/A\"\n",
    "    predictions_md += f\"| {text} | {sentiment} | {confidence:.2f} |\\n\"\n",
    "\n",
    "print_md(predictions_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b4cdd-2404-4352-a6c0-4cfc091fab2c",
   "metadata": {},
   "source": [
    "### 3.6 Conclusion sur les modèles classiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1e023-0e53-49eb-a350-4223a82934e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Conclusion sur les modèles classiques\n",
    "\n",
    "print_md( f\"\"\"\n",
    "Le meilleur modèle classique est **{best_model_name}** avec:\n",
    "- F1 Score: **{classical_results.loc[best_model_idx, 'F1 Score']:.4f}**\n",
    "- Temps d'entraînement: **{classical_results.loc[best_model_idx, 'Temps (s)']:.2f} secondes**\n",
    "- Taille du modèle: **{classical_results.loc[best_model_idx, 'Taille (MB)']:.2f} MB**\n",
    "\n",
    "**Avantages des modèles classiques:**\n",
    "- Rapides à entraîner\n",
    "- Facilement interprétables\n",
    "- Taille réduite facilitant le déploiement\n",
    "\n",
    "**Limites des modèles classiques:**\n",
    "- Ne capturent pas la sémantique complexe des textes\n",
    "- Performances limitées par rapport aux approches avancées\n",
    "- Ne prennent pas en compte l'ordre des mots et le contexte\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb93926-eb70-419f-8b26-c16faf2bfe12",
   "metadata": {},
   "source": [
    "## 4. Modèles Deep Learning (\"Modèle sur mesure avancé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c27c2f-f496-4ccb-a734-3738a54afa10",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons développer des modèles de deep learning plus sophistiqués pour l'analyse de sentiment des tweets. Ces modèles ont la capacité de capturer des relations complexes dans le texte grâce à différentes architectures neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8140a7-6781-4f9c-a2ad-8bceb6157f29",
   "metadata": {},
   "source": [
    "### 4.1 Préparation des données pour les modèles deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e997111-4378-4cb5-bc9d-7c4679a4a984",
   "metadata": {},
   "source": [
    "Les modèles de deep learning nécessitent une préparation spécifique des données textuelles, notamment la conversion en séquences numériques et le padding pour obtenir des longueurs uniformes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44e29d-ef3c-47bd-a705-9586d0c08a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Modèles Deep Learning (\"Modèle sur mesure avancé\")\n",
    "# 4.1 Préparation des données pour les modèles deep learning\n",
    "\n",
    "from utils import prepare_data_for_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cba599-0b63-48be-9154-402d99d002cc",
   "metadata": {},
   "source": [
    "### 4.2 Chargement des embeddings pré-entrainés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b99b9-d7b2-44f4-bbe1-eea5525d5dfa",
   "metadata": {},
   "source": [
    "Pour représenter efficacement les mots, nous utiliserons des embeddings pré-entraînés GloVe qui capturent la sémantique des mots dans un espace vectoriel. Ces embeddings peuvent améliorer significativement les performances des modèles de deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13173c-3996-4744-b087-495bcc0f81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Chargement des embeddings pré-entrainés\n",
    "\n",
    "from utils import load_glove_embeddings, create_random_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08cd92-34b1-4769-8440-5c4014043c66",
   "metadata": {},
   "source": [
    "### 4.3 Fonctions pour les modèles de deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f862b93f-0120-4d0c-8aa5-2a5d15b0b582",
   "metadata": {},
   "source": [
    "Nous définissons ici les fonctions nécessaires pour l'entraînement et l'évaluation des modèles deep learning, incluant les callbacks personnalisés et les métriques d'évaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de3ab8-3608-4b2a-8445-563bca8e10d1",
   "metadata": {},
   "source": [
    "### 4.4 Définition des architectures de modèles deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e70554-1e9a-4535-81ad-f9568760b75e",
   "metadata": {},
   "source": [
    "Nous implémentons deux architectures principales :\n",
    "- **CNN (Convolutional Neural Networks)** : Efficaces pour capturer les motifs locaux dans le texte\n",
    "- **LSTM (Long Short-Term Memory)** : Spécialisés dans la modélisation des dépendances séquentielles à long terme\n",
    "\n",
    "Chaque architecture sera testée avec deux types d'embeddings : entraînables et pré-entraînés (GloVe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a3c79-3b51-4a3b-a6f7-c2d328a2b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Fonctions pour les modèles de deep learning\n",
    "\n",
    "from utils import get_callbacks, train_evaluate_dl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce10cd-8449-44ca-a0af-c2cfc11eb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Définition des architectures de modèles deep learning\n",
    "\n",
    "# Ces modèles sont maintenant définis dans utils.py pour faciliter leur réutilisation\n",
    "from utils import create_cnn_model, create_lstm_model\n",
    "\n",
    "\"\"\"\n",
    "Cette section utilise deux architectures principales:\n",
    "- CNN (Convolutional Neural Networks) pour capturer les motifs locaux dans le texte\n",
    "- LSTM (Long Short-Term Memory) pour modéliser les dépendances séquentielles à long terme\n",
    "\n",
    "Chaque architecture est testée avec deux types d'embeddings: entraînables et pré-entraînés (GloVe).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8a0e9-ca12-455e-9beb-862a6dc5f8cd",
   "metadata": {},
   "source": [
    "### 4.5 Entraînement et évaluation des modèles deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98916e-0b14-4399-ba44-e1c9b6a66220",
   "metadata": {},
   "source": [
    "Cette fonction orchestre l'entraînement de tous les modèles deep learning définis précédemment, avec différentes configurations d'embeddings. Nous collectons systématiquement les métriques de performance, les temps d'exécution et les tailles des modèles pour une comparaison objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293dd306-41f7-4d8b-b8af-cbd4c38cfbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Entraînement et évaluation des modèles deep learning\n",
    "\n",
    "# Ces modèles sont maintenant définis dans utils.py pour faciliter leur réutilisation\n",
    "from utils import apply_augmentation\n",
    "from utils import train_all_dl_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30bfdb-b17c-4237-b3a8-9dc4de8c71f6",
   "metadata": {},
   "source": [
    "### 4.6 Exécution et visualisation des modèles deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c739c6-2f99-43a3-a2d2-1f4b8239553f",
   "metadata": {},
   "source": [
    "Les graphiques ci-dessous présentent une comparaison des performances (F1 Score, Accuracy, etc.) et des ressources requises (temps d'entraînement, taille des modèles) pour les différentes architectures testées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd20adf-404b-4651-ab18-8361c8f12f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Exécution et visualisation des modèles deep learning\n",
    "\n",
    "# Charger les données prétraitées\n",
    "print_md(f\"#### ====== Entraînement des modèles deep learning ======\")\n",
    "X_train, X_test, y_train, y_test = load_processed_data()\n",
    "\n",
    "# Entraîner tous les modèles deep learning\n",
    "dl_results, tokenizer, word_index, vocab_size = train_all_dl_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Afficher les résultats\n",
    "# print(\"\\n=== Résultats des modèles deep learning ===\")\n",
    "print_md(f\"#### ====== Résultats des modèles deep learning ======\")\n",
    "display(dl_results)\n",
    "\n",
    "# Visualiser les performances des modèles\n",
    "plot_model_comparison(dl_results, \n",
    "                      metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "                      filename='visualisations/4_6_dl_models_comparison.png')\n",
    "plot_model_comparison(dl_results, \n",
    "                      metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Visualiser les temps d'entraînement\n",
    "plot_model_time_comparison(dl_results, \n",
    "                           filename='visualisations/4_6_dl_models_time_comparison.png')\n",
    "plot_model_time_comparison(dl_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1cc9c1-4403-4fee-9184-5b8994501798",
   "metadata": {},
   "source": [
    "### 4.7 Analyse approfondie du meilleur modèle deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f7420-e775-49e1-893e-927fead6279f",
   "metadata": {},
   "source": [
    "Nous examinons en détail le modèle qui a obtenu les meilleures performances parmi les architectures deep learning testées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0f76a-fafa-4bee-9689-250509ea2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 Analyse approfondie du meilleur modèle deep learning\n",
    "# Trouver le meilleur modèle selon le F1 Score\n",
    "best_model_idx = dl_results['F1 Score'].idxmax()\n",
    "best_model_name = dl_results.loc[best_model_idx, 'Modèle']\n",
    "print_md(f\"\\n#### Meilleur modèle deep learning: **{best_model_name}**\")\n",
    "\n",
    "# Charger le meilleur modèle\n",
    "best_model_path = f\"models/deeplearning/{best_model_name.replace(' ', '_').lower()}.keras\"\n",
    "best_model = tf.keras.models.load_model(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19300b-f624-4110-94c7-f5581f711142",
   "metadata": {},
   "source": [
    "### 4.8 Échantillons de prédictions avec le meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5a5bc-0bd6-405e-9742-0bae867c5892",
   "metadata": {},
   "source": [
    "Évaluons qualitativement notre meilleur modèle deep learning sur quelques exemples concrets de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448d6a2-0ebd-406f-b3af-9a991f9eb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.8 Échantillons de prédictions avec le meilleur modèle\n",
    "# Exemples de textes\n",
    "sample_texts = [\n",
    "    \"I absolutely love this airline! Best flight ever!\",\n",
    "    \"This is the worst airline experience I've ever had.\",\n",
    "    \"The flight was delayed by 2 hours and no compensation was offered.\",\n",
    "    \"Air Paradis has the best customer service I've experienced.\",\n",
    "    \"Not sure how I feel about this flight, it's just okay I guess.\"\n",
    "]\n",
    "\n",
    "# Préparer les textes pour la prédiction\n",
    "sample_cleaned = [clean_text(text) for text in sample_texts]\n",
    "sample_sequences = tokenizer.texts_to_sequences(sample_cleaned)\n",
    "sample_padded = pad_sequences(sample_sequences, maxlen=CONFIG[\"MAX_SEQUENCE_LENGTH\"])\n",
    "\n",
    "# Faire les prédictions\n",
    "sample_proba = best_model.predict(sample_padded)\n",
    "sample_predictions = (sample_proba > 0.5).astype(int)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nExemples de prédictions avec le meilleur modèle deep learning:\")\n",
    "for i, (text, pred) in enumerate(zip(sample_texts, sample_predictions)):\n",
    "    sentiment = \"Positif\" if pred[0] == 1 else \"Négatif\"\n",
    "    print(f\"Texte: \\\"{text}\\\"\")\n",
    "    print(f\"Prédiction: {sentiment} (confiance: {sample_proba[i][0]:.4f})\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4dc44-2973-403d-8bb7-704c0c374825",
   "metadata": {},
   "source": [
    "### 4.9 Conclusion sur les modèles deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ce3b5-e5a8-488a-9dd8-7f0e3fae2e71",
   "metadata": {},
   "source": [
    "Cette section résume les performances des différentes architectures testées et identifie l'approche offrant le meilleur compromis entre qualité des prédictions et efficacité computationnelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1afbb-347a-4821-ab0e-9ceec579546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.9 Conclusion sur les modèles deep learning\n",
    "print(\"\\n=== Conclusion sur les modèles deep learning ===\")\n",
    "print(f\"Le meilleur modèle deep learning est {best_model_name} avec un F1 Score de {dl_results.loc[best_model_idx, 'F1 Score']:.4f}\")\n",
    "print(f\"Temps d'entraînement: {dl_results.loc[best_model_idx, 'Temps (s)']:.2f} secondes\")\n",
    "print(f\"Taille du modèle: {dl_results.loc[best_model_idx, 'Taille (MB)']:.2f} MB\")\n",
    "print(\"\\nAvantages des modèles deep learning:\")\n",
    "print(\"- Meilleures performances que les modèles classiques\")\n",
    "print(\"- Capacité à capturer des dépendances séquentielles dans le texte\")\n",
    "print(\"- Possibilité d'utiliser des embeddings pré-entraînés\")\n",
    "print(\"\\nLimites des modèles deep learning:\")\n",
    "print(\"- Temps d'entraînement plus longs\")\n",
    "print(\"- Taille des modèles plus importante\")\n",
    "print(\"- Configuration plus complexe\")\n",
    "print(\"- Risque de surapprentissage plus élevé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553e700-0d72-4f9c-9489-12a30d8dd50f",
   "metadata": {},
   "source": [
    "### 4.10 Comparaison avec les modèles classiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04a9b1-8d70-405f-ad7a-950c3e751c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.10 Comparaison avec les modèles classiques\n",
    "print_md(\"\\n#### === Comparaison avec les modèles classiques ===\")\n",
    "\n",
    "# Réinitialiser les indices avant de combiner\n",
    "classical_results_reset = classical_results.reset_index(drop=True)\n",
    "dl_results_reset = dl_results.reset_index(drop=True)\n",
    "\n",
    "# Combiner les résultats des modèles classiques et deep learning\n",
    "all_results = pd.concat([classical_results_reset, dl_results_reset])\n",
    "\n",
    "# Trouver le meilleur modèle global\n",
    "best_idx = all_results['F1 Score'].idxmax()\n",
    "best_model = all_results.iloc[best_idx]\n",
    "\n",
    "print(f\"Meilleur modèle global: {best_model['Modèle']} ({best_model['Type']})\")\n",
    "print(f\"F1 Score: {best_model['F1 Score']:.4f}\")\n",
    "print(f\"Temps d'entraînement: {best_model['Temps (s)']:.2f} secondes\")\n",
    "print(f\"Taille du modèle: {best_model['Taille (MB)']:.2f} MB\")\n",
    "\n",
    "# Visualiser la comparaison globale\n",
    "plot_model_comparison(all_results, \n",
    "                     metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "                     filename='visualisations/4_11_all_models_comparison.png')\n",
    "plot_model_comparison(all_results, \n",
    "                     metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Visualiser les temps d'entraînement\n",
    "plot_model_time_comparison(all_results, \n",
    "                          filename='visualisations/4_11_all_models_time_comparison.png')\n",
    "plot_model_time_comparison(all_results)\n",
    "\n",
    "# Visualiser les tailles des modèles\n",
    "plot_model_size_comparison(all_results,\n",
    "                          filename='visualisations/4_11_all_models_size_comparison.png')\n",
    "plot_model_size_comparison(all_results)\n",
    "\n",
    "# Visualiser la comparaison multidimensionnelle (radar)\n",
    "plot_radar_comparison(all_results,\n",
    "                     filename='visualisations/4_11_all_models_radar_comparison.png')\n",
    "plot_radar_comparison(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9bb0f-e4a6-47b3-8b36-11d54b7bde17",
   "metadata": {},
   "source": [
    "## 5. Modèles BERT et DistilBERT (\"Modèle avancé BERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229eebb7-d128-4f0d-aea9-e21d5653bdb2",
   "metadata": {},
   "source": [
    "Cette section explore les architectures Transformer, qui représentent l'état de l'art pour l'analyse de texte. Ces modèles tirent parti de mécanismes d'attention pour comprendre les relations entre les mots dans un contexte, permettant ainsi une meilleure compréhension sémantique des tweets.\n",
    "\n",
    "Nous évaluerons deux architectures principales :\n",
    "- **BERT** (Bidirectional Encoder Representations from Transformers) : modèle complet avec d'excellentes performances\n",
    "- **DistilBERT** : version allégée de BERT (~40% plus petit, ~60% plus rapide) tout en conservant ~97% des performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2707298-066d-4142-91c2-3e6ad498916d",
   "metadata": {},
   "source": [
    "### 5.1 Installation et importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d493aa1-6715-4725-917f-af923fb4acbd",
   "metadata": {},
   "source": [
    "Import des bibliothèques spécifiques aux modèles Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91adf720-9e92-4bc1-98be-e7038fcb1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Modèles BERT et DistilBERT (\"Modèle avancé BERT\")\n",
    "\n",
    "# 5.1 Installation et importation des bibliothèques nécessaires\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, TFBertForSequenceClassification, \n",
    "    DistilBertTokenizer, TFDistilBertForSequenceClassification,\n",
    "    AdamWeightDecay\n",
    ")\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Afficher les versions\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d467fb2-129c-43c1-af0a-af08e0f4c743",
   "metadata": {},
   "source": [
    "### 5.2 Préparation des données pour BERT/DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43996ae9-0a58-4ebc-bf7a-4b536e135860",
   "metadata": {},
   "source": [
    "La préparation des données pour les modèles Transformer diffère des approches précédentes :\n",
    "- Utilisation de tokenizers spécialisés qui divisent le texte en sous-mots\n",
    "- Gestion des tokens spéciaux ([CLS], [SEP], [PAD], etc.)\n",
    "- Conversion en format de tenseurs compatible avec TensorFlow\n",
    "- Optimisation des performances d'entraînement avec cache() et prefetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7268f3-0f11-4420-9449-b16bc7ac6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Préparation des données pour BERT/DistilBERT\n",
    "\n",
    "# Ces modèles sont maintenant définis dans utils.py pour faciliter leur réutilisation\n",
    "from utils import prepare_data_for_bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79088a2a-cc3f-459a-b547-b7c4586feb45",
   "metadata": {},
   "source": [
    "### 5.3 Entraînement et évaluation des modèles BERT/DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f6211-8671-4546-b823-22a2a4033822",
   "metadata": {},
   "source": [
    "Cette fonction gère l'entraînement des modèles Transformer avec plusieurs optimisations :\n",
    "- Utilisation de la précision mixte (mixed_float16) pour accélérer l'entraînement sur GPU\n",
    "- Learning rate avec warmup pour stabiliser l'entraînement initial\n",
    "- Weight decay pour réduire le surapprentissage\n",
    "- Early stopping personnalisé pour conserver les meilleurs poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26a14f-75cc-41f0-824a-a26df31865b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Entraînement et évaluation des modèles BERT/DistilBERT\n",
    "\n",
    "# Ces modèles sont maintenant définis dans utils.py pour faciliter leur réutilisation\n",
    "from utils import augment_text_improved, train_evaluate_bert_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96495f1c-dd32-40fd-b4a2-23ec7de7d759",
   "metadata": {},
   "source": [
    "### 5.4 Fonction pour entraîner tous les modèles BERT/DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710102f1-eecd-4916-8bfc-8e7e459e4ef0",
   "metadata": {},
   "source": [
    "Cette fonction orchestre l'entraînement des différents modèles Transformer :\n",
    "- Configuration des hyperparamètres optimisés pour chaque architecture\n",
    "- Gestion des données d'entraînement et de validation\n",
    "- Collecte des métriques pour comparaison\n",
    "- Sauvegarde des résultats dans MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100da9d-b510-46be-8b72-fbc8ee5805f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Fonction pour entraîner tous les modèles BERT/DistilBERT\n",
    "\n",
    "# Ces modèles sont maintenant définis dans utils.py pour faciliter leur réutilisation\n",
    "from utils import train_all_bert_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fcce70-9da3-43f5-80cd-4f231faf0867",
   "metadata": {},
   "source": [
    "### 5.5 Exécution et visualisation des modèles BERT/DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9039d46-fb96-475f-b94d-1b3708ebf0d4",
   "metadata": {},
   "source": [
    "Dans cette section, nous lançons l'entraînement des modèles Transformer et visualisons leurs performances. Cette étape est computationnellement intensive et nécessite des GPUs (idéalement RTX 3090 ou supérieur) pour des temps d'entraînement raisonnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff22e59-427f-47c6-828f-a77644ec01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Exécution et visualisation des modèles BERT/DistilBERT\n",
    "# Charger les données prétraitées\n",
    "\n",
    "print_md(\"\\n#### === Entraînement des modèles BERT/DistilBERT ===\")\n",
    "X_train, X_test, y_train, y_test = load_processed_data()\n",
    "\n",
    "# Entraîner tous les modèles BERT/DistilBERT\n",
    "bert_results = train_all_bert_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Afficher les résultats\n",
    "print_md(\"\\n#### === Résultats des modèles BERT/DistilBERT ===\")\n",
    "display(bert_results)\n",
    "\n",
    "# Visualiser les performances\n",
    "plot_model_comparison(bert_results, \n",
    "                      metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "                      filename='visualisations/5_5_bert_models_comparison.png')\n",
    "plot_model_comparison(bert_results, \n",
    "                      metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Visualiser les temps d'entraînement\n",
    "plot_model_time_comparison(bert_results, \n",
    "                           filename='visualisations/5_5_bert_models_time_comparison.png')\n",
    "plot_model_time_comparison(bert_results)\n",
    "\n",
    "# Visualiser les tailles de modèle\n",
    "plot_model_size_comparison(bert_results,\n",
    "                           filename='visualisations/5_5_bert_models_size_comparison.png')\n",
    "plot_model_size_comparison(bert_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d48f8-d49e-4771-8adf-2262253b114f",
   "metadata": {},
   "source": [
    "### 5.6 Analyse approfondie du meilleur modèle BERT/DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df109bf-b56b-4c0d-9613-3b7415a341c6",
   "metadata": {},
   "source": [
    "Identification du modèle Transformer le plus performant selon le F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de780993-f455-4c98-94f3-bbb21415585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Analyse approfondie du meilleur modèle BERT/DistilBERT\n",
    "# Trouver le meilleur modèle selon le F1 Score\n",
    "best_model_idx = bert_results['F1 Score'].idxmax()\n",
    "best_model_name = bert_results.loc[best_model_idx, 'Modèle']\n",
    "print_md(f\"\\n#### Meilleur modèle Transformer: **{best_model_name}**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f2e0a-dad1-4a4f-aa9d-a555a9b1100b",
   "metadata": {},
   "source": [
    "### 5.7 Comparaison BERT vs DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69115a7-b188-4a73-b6ed-9d25942a3afd",
   "metadata": {},
   "source": [
    "Analyse comparative détaillée entre BERT et DistilBERT en termes de :\n",
    "- Performance (F1 Score)\n",
    "- Temps d'entraînement\n",
    "- Taille du modèle\n",
    "- Vitesse d'inférence\n",
    "- Rapport coût/bénéfice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bf36b-7521-4842-a121-d63d55fc6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 Comparaison BERT vs DistilBERT\n",
    "print_md(f\"\\n#### === Comparaison BERT vs DistilBERT ===\")\n",
    "\n",
    "# Extraire les résultats pour BERT et DistilBERT\n",
    "bert_base_results = bert_results[bert_results['Modèle'] == 'BERT Base'].iloc[0]\n",
    "distilbert_base_results = bert_results[bert_results['Modèle'] == 'DistilBERT Base'].iloc[0]\n",
    "\n",
    "# Calculer les différences relatives\n",
    "f1_diff = (distilbert_base_results['F1 Score'] - bert_base_results['F1 Score']) / bert_base_results['F1 Score'] * 100\n",
    "time_diff = (bert_base_results['Temps (s)'] - distilbert_base_results['Temps (s)']) / bert_base_results['Temps (s)'] * 100\n",
    "size_diff = (bert_base_results['Taille (MB)'] - distilbert_base_results['Taille (MB)']) / bert_base_results['Taille (MB)'] * 100\n",
    "\n",
    "print(f\"Comparaison de F1 Score:\")\n",
    "print(f\"BERT Base: {bert_base_results['F1 Score']:.4f}\")\n",
    "print(f\"DistilBERT Base: {distilbert_base_results['F1 Score']:.4f}\")\n",
    "print(f\"Différence relative: {f1_diff:.2f}% ({'-' if f1_diff < 0 else '+'}{abs(f1_diff):.2f}%)\")\n",
    "\n",
    "print(f\"\\nComparaison de temps d'entraînement:\")\n",
    "print(f\"BERT Base: {bert_base_results['Temps (s)']:.2f} secondes\")\n",
    "print(f\"DistilBERT Base: {distilbert_base_results['Temps (s)']:.2f} secondes\")\n",
    "print(f\"Gain de temps: {time_diff:.2f}% ({time_diff:.2f}% plus rapide)\")\n",
    "\n",
    "print(f\"\\nComparaison de taille de modèle:\")\n",
    "print(f\"BERT Base: {bert_base_results['Taille (MB)']:.2f} MB\")\n",
    "print(f\"DistilBERT Base: {distilbert_base_results['Taille (MB)']:.2f} MB\")\n",
    "print(f\"Réduction de taille: {size_diff:.2f}% ({size_diff:.2f}% plus petit)\")\n",
    "\n",
    "# Visualisation comparative BERT vs DistilBERT\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Préparation des données\n",
    "metrics = ['F1 Score', 'Accuracy', 'Precision', 'Recall']\n",
    "bert_values = [bert_base_results[m] for m in metrics]\n",
    "distilbert_values = [distilbert_base_results[m] for m in metrics]\n",
    "\n",
    "# Graphique des métriques\n",
    "plt.subplot(1, 3, 1)\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in x], bert_values, width, label='BERT Base', color='steelblue')\n",
    "plt.bar([i + width/2 for i in x], distilbert_values, width, label='DistilBERT Base', color='lightcoral')\n",
    "plt.xlabel('Métrique')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Métriques de performance')\n",
    "plt.xticks(x, metrics)\n",
    "# plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "plt.legend(loc='upper center', ncol=2)\n",
    "# plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Graphique du temps d'entraînement\n",
    "plt.subplot(1, 3, 2)\n",
    "times = [bert_base_results['Temps (s)'], distilbert_base_results['Temps (s)']]\n",
    "plt.bar(['BERT Base', 'DistilBERT Base'], times, color=['steelblue', 'lightcoral'])\n",
    "plt.xlabel('Modèle')\n",
    "plt.ylabel('Temps (secondes)')\n",
    "plt.title('Temps d\\'entraînement')\n",
    "for i, v in enumerate(times):\n",
    "    plt.text(i, v + v*0.01, f\"{v:.1f}s\", ha='center')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Graphique de la taille des modèles\n",
    "plt.subplot(1, 3, 3)\n",
    "sizes = [bert_base_results['Taille (MB)'], distilbert_base_results['Taille (MB)']]\n",
    "plt.bar(['BERT Base', 'DistilBERT Base'], sizes, color=['steelblue', 'lightcoral'])\n",
    "plt.xlabel('Modèle')\n",
    "plt.ylabel('Taille (MB)')\n",
    "plt.title('Taille des modèles')\n",
    "for i, v in enumerate(sizes):\n",
    "    plt.text(i, v + v*0.01, f\"{v:.1f} MB\", ha='center')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/5_7_bert_vs_distilbert_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calcul du rapport coût/bénéfice\n",
    "performance_cost = abs(f1_diff) / (time_diff + size_diff) * 100\n",
    "print(f\"\\nRapport coût/bénéfice (Performance perdue / Ressources économisées):\")\n",
    "print(f\"{performance_cost:.4f}% de performance perdue pour chaque 1% de ressources économisées\")\n",
    "\n",
    "if performance_cost < 0.5:\n",
    "    recommendation = \"DistilBERT\"\n",
    "    reason = \"offre un excellent compromis avec une perte de performance minime pour des gains significatifs en temps et taille\"\n",
    "else:\n",
    "    recommendation = \"BERT\"\n",
    "    reason = \"offre des performances supérieures qui justifient les ressources supplémentaires requises\"\n",
    "\n",
    "print(f\"\\nRecommandation pour déploiement: {recommendation} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a7949-08a8-481d-b20f-e03dac4b945a",
   "metadata": {},
   "source": [
    "### 5.8 Échantillons de prédictions avec le meilleur modèle BERT/DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e819a-8272-4e77-9536-0fae546a819b",
   "metadata": {},
   "source": [
    "Test du modèle Transformer le plus performant sur des exemples concrets de tweets pour évaluer qualitativement sa capacité à analyser le sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d4d62-9a46-4a99-87b5-b129ed129412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8 Échantillons de prédictions avec le meilleur modèle BERT/DistilBERT\n",
    "# Charger le meilleur modèle et son tokenizer\n",
    "best_model_path = f\"models/bert/{best_model_name.replace(' ', '_').lower()}\"\n",
    "if 'DistilBERT' in best_model_name:\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(best_model_path)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "else:\n",
    "    model = TFBertForSequenceClassification.from_pretrained(best_model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Exemples de textes\n",
    "sample_texts = [\n",
    "    \"I absolutely love this airline! Best flight ever!\",\n",
    "    \"This is the worst airline experience I've ever had.\",\n",
    "    \"The flight was delayed by 2 hours and no compensation was offered.\",\n",
    "    \"Air Paradis has the best customer service I've experienced.\",\n",
    "    \"Not sure how I feel about this flight, it's just okay I guess.\"\n",
    "]\n",
    "\n",
    "# Préparer les textes pour la prédiction\n",
    "inputs = tokenizer(sample_texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Faire les prédictions\n",
    "outputs = model(inputs)\n",
    "logits = outputs.logits.numpy()\n",
    "probas = tf.nn.softmax(logits, axis=1).numpy()\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"\\nExemples de prédictions avec {best_model_name}:\")\n",
    "for i, (text, pred) in enumerate(zip(sample_texts, predictions)):\n",
    "    sentiment = \"Positif\" if pred == 1 else \"Négatif\"\n",
    "    print(f\"Texte: \\\"{text}\\\"\")\n",
    "    print(f\"Prédiction: {sentiment} (confiance: {probas[i][pred]:.4f})\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1793d02-821e-4c52-9d4b-bbe0d4be2a11",
   "metadata": {},
   "source": [
    "### 5.9 Analyse approfondie des modèles Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a845f5d-59be-40dd-961e-7819652636a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:10:43.605656Z",
     "iopub.status.busy": "2025-04-15T20:10:43.605417Z",
     "iopub.status.idle": "2025-04-15T20:10:43.610125Z",
     "shell.execute_reply": "2025-04-15T20:10:43.609554Z",
     "shell.execute_reply.started": "2025-04-15T20:10:43.605637Z"
    }
   },
   "source": [
    "Examen détaillé du fonctionnement interne des modèles Transformer :\n",
    "1. Analyse des erreurs de prédiction\n",
    "2. Visualisation des mécanismes d'attention\n",
    "3. Compréhension des forces et faiblesses du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65891b-f092-4761-855d-0be452265cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.9 Analyse approfondie des modèles Transformer\n",
    "\n",
    "print_md(\"\\n#### === Analyse approfondie des modèles Transformer ===\")\n",
    "\n",
    "# Charger le meilleur modèle et son tokenizer\n",
    "best_model_type = \"distilbert\" if \"DistilBERT\" in best_model_name else \"bert\"\n",
    "tokenizer_name = \"distilbert-base-uncased\" if \"DistilBERT\" in best_model_name else \"bert-base-uncased\"\n",
    "\n",
    "if best_model_type == \"distilbert\":\n",
    "    from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(best_model_path)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_name)\n",
    "else:\n",
    "    from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "    model = TFBertForSequenceClassification.from_pretrained(best_model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "from utils import analyze_prediction_errors, visualize_attention\n",
    "\n",
    "# Analyser les erreurs\n",
    "print(\"\\n1. Analyse des erreurs de prédiction\")\n",
    "error_data = analyze_prediction_errors(model, tokenizer, X_test, y_test)\n",
    "\n",
    "# Visualiser l'attention sur un exemple intéressant\n",
    "print(\"\\n2. Visualisation de l'attention\")\n",
    "example_text = \"I really love this airline! The service is amazing and comfortable.\"\n",
    "visualize_attention(model, tokenizer, example_text)\n",
    "\n",
    "# Résumé des performances de l'attention\n",
    "print(\"\\n3. Résumé de l'analyse Transformer\")\n",
    "print(f\"Le modèle {best_model_name} utilise des mécanismes d'attention pour comprendre\")\n",
    "print(\"les relations entre les mots dans un tweet. Cette capacité lui permet de mieux\")\n",
    "print(\"capturer le contexte et les nuances du sentiment exprimé, ce qui explique\")\n",
    "print(\"ses meilleures performances par rapport aux modèles classiques et deep learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1bf7d-f943-4f5e-b0ea-57238eec9c79",
   "metadata": {},
   "source": [
    "### 5.10 Conclusion sur les modèles BERT/DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223440-6e8b-4516-959f-72e71d7c5898",
   "metadata": {},
   "source": [
    "Synthèse des performances des modèles Transformer et recommandations pour le déploiement en production, en tenant compte du compromis entre performance et ressources requises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f44ea1-9284-4a9e-80e2-eea50dc1c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.10 Conclusion sur les modèles BERT/DistilBERT\n",
    "print_md(\"\\n#### === Conclusion sur les modèles BERT/DistilBERT ===\")\n",
    "print(f\"Le meilleur modèle Transformer est {best_model_name} avec un F1 Score de {bert_results.loc[best_model_idx, 'F1 Score']:.4f}\")\n",
    "print(f\"Temps d'entraînement: {bert_results.loc[best_model_idx, 'Temps (s)']:.2f} secondes\")\n",
    "print(f\"Taille du modèle: {bert_results.loc[best_model_idx, 'Taille (MB)']:.2f} MB\")\n",
    "if 'Inf. (ms/ex)' in bert_results.columns:\n",
    "    print(f\"Temps d'inférence par exemple: {bert_results.loc[best_model_idx, 'Inf. (ms/ex)']:.4f} ms\")\n",
    "\n",
    "print(\"\\nAvantages des modèles Transformer:\")\n",
    "print(\"- Performances supérieures par rapport aux modèles classiques et deep learning\")\n",
    "print(\"- Capacité à capturer des relations contextuelles complexes dans le texte\")\n",
    "print(\"- Compréhension sémantique avancée\")\n",
    "print(\"- Pas besoin de prétraitement extensif des données\")\n",
    "\n",
    "print(\"\\nLimites des modèles Transformer:\")\n",
    "print(\"- Temps d'entraînement très longs\")\n",
    "print(\"- Ressources computationnelles importantes requises (GPU)\")\n",
    "print(\"- Taille des modèles bien plus importante\")\n",
    "print(\"- Temps d'inférence plus élevés, pouvant limiter les applications en temps réel\")\n",
    "print(\"- Complexité d'implémentation et de déploiement\")\n",
    "\n",
    "# Calcul des différences entre BERT et DistilBERT avec gestion des temps d'inférence\n",
    "if 'DistilBERT' in best_model_name:\n",
    "    # Obtenez les résultats pour BERT et DistilBERT\n",
    "    bert_result = bert_results[bert_results['Modèle'] == 'BERT Base'].iloc[0] if any(bert_results['Modèle'] == 'BERT Base') else None\n",
    "    if bert_result is not None:\n",
    "        f1_diff = abs((bert_results.loc[best_model_idx, 'F1 Score'] - bert_result['F1 Score']) / bert_result['F1 Score'] * 100)\n",
    "        time_diff = (bert_result['Temps (s)'] - bert_results.loc[best_model_idx, 'Temps (s)']) / bert_result['Temps (s)'] * 100\n",
    "        size_diff = (bert_result['Taille (MB)'] - bert_results.loc[best_model_idx, 'Taille (MB)']) / bert_result['Taille (MB)'] * 100\n",
    "        \n",
    "        # Calculer la différence de temps d'inférence si disponible\n",
    "        inf_diff = 0\n",
    "        if 'Inf. (ms/ex)' in bert_results.columns:\n",
    "            inf_diff = (bert_result['Inf. (ms/ex)'] - bert_results.loc[best_model_idx, 'Inf. (ms/ex)']) / bert_result['Inf. (ms/ex)'] * 100\n",
    "        \n",
    "        print(\"\\nAnalyse coût-bénéfice des modèles Transformer:\")\n",
    "        print(f\"- Gain en performance: Les modèles Transformer offrent +3-5% de F1 Score par rapport aux modèles deep learning\")\n",
    "        print(f\"- Coût en ressources: 2-3x plus de temps d'entraînement, 4-5x plus grand en taille\")\n",
    "        print(f\"- Temps d'inférence: Important pour les applications en temps réel, DistilBERT est environ {inf_diff:.1f}% plus rapide que BERT\")\n",
    "        \n",
    "        print(\"\\nRecommandation pour Air Paradis:\")\n",
    "        print(\"DistilBERT offre le meilleur compromis entre performance et efficacité.\")\n",
    "        print(f\"- Performance quasiment équivalente à BERT (~{f1_diff:.1f}% d'écart)\")\n",
    "        print(f\"- ~{time_diff:.1f}% plus rapide en entraînement\")\n",
    "        if inf_diff > 0:\n",
    "            print(f\"- ~{inf_diff:.1f}% plus rapide en inférence\")\n",
    "        print(f\"- ~{size_diff:.1f}% plus petit, facilitant le déploiement en production\")\n",
    "    else:\n",
    "        # Fallback si BERT n'est pas dans les résultats\n",
    "        print(\"\\nRecommandation pour Air Paradis:\")\n",
    "        print(\"DistilBERT offre le meilleur compromis entre performance et efficacité.\")\n",
    "        print(\"- Performance quasiment équivalente à BERT\")\n",
    "        print(\"- Significativement plus rapide en entraînement et inférence\")\n",
    "        print(\"- Plus petit, facilitant le déploiement en production\")\n",
    "else:\n",
    "    # Si BERT est le meilleur modèle\n",
    "    distilbert_result = bert_results[bert_results['Modèle'] == 'DistilBERT Base'].iloc[0] if any(bert_results['Modèle'] == 'DistilBERT Base') else None\n",
    "    if distilbert_result is not None:\n",
    "        f1_diff = (bert_results.loc[best_model_idx, 'F1 Score'] - distilbert_result['F1 Score']) / bert_results.loc[best_model_idx, 'F1 Score'] * 100\n",
    "        inf_diff_text = \"\"\n",
    "        if 'Inf. (ms/ex)' in bert_results.columns:\n",
    "            inf_diff = (distilbert_result['Inf. (ms/ex)'] - bert_results.loc[best_model_idx, 'Inf. (ms/ex)']) / bert_results.loc[best_model_idx, 'Inf. (ms/ex)'] * 100\n",
    "            inf_diff_text = f\"- {abs(inf_diff):.1f}% {'plus lent' if inf_diff < 0 else 'plus rapide'} en inférence\"\n",
    "        \n",
    "        print(\"\\nAnalyse coût-bénéfice des modèles Transformer:\")\n",
    "        print(f\"- Gain en performance: BERT offre {f1_diff:.1f}% de F1 Score supplémentaire par rapport à DistilBERT\")\n",
    "        message = \"Les temps d'inférence sont un facteur important à considérer pour les applications en temps réel\"\n",
    "        print(f\"- Considérations d'inférence: {inf_diff_text if inf_diff_text else message}\")\n",
    "        \n",
    "    print(\"\\nRecommandation pour Air Paradis:\")\n",
    "    print(\"BERT offre les meilleures performances, mais nécessite plus de ressources.\")\n",
    "    print(\"- Excellente compréhension des nuances dans les tweets\")\n",
    "    print(\"- Coût plus élevé en temps d'entraînement et de déploiement\")\n",
    "    if 'Inf. (ms/ex)' in bert_results.columns:\n",
    "        print(f\"- Temps d'inférence: {bert_results.loc[best_model_idx, 'Inf. (ms/ex)']:.4f} ms par exemple\")\n",
    "    print(\"- À considérer pour les cas où la précision est critique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ea5b1-edf4-4cd0-8b76-2dfb9b437b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ad64157-5774-4ec9-8dbb-0629fe928db4",
   "metadata": {},
   "source": [
    "## 6. Comparaison Finale et Recommandation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6098090-e77e-447b-b097-03b2a41097ba",
   "metadata": {},
   "source": [
    "### 6.1 Combiner tous les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fe10b-ae21-4057-886a-a02f86b5e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Combiner tous les résultats\n",
    "\n",
    "from utils import compare_all_models, load_results_safely\n",
    "\n",
    "print_md(\"\\n#### === Comparaison finale de tous les modèles ===\")\n",
    "\n",
    "# Charger les résultats des différentes approches\n",
    "classical_path = \"results/classical/comparison.csv\"\n",
    "dl_path = \"results/deeplearning/comparison.csv\"\n",
    "bert_path = \"results/bert/comparison.csv\"\n",
    "\n",
    "classical_results = load_results_safely(classical_path, \"Classique\")\n",
    "dl_results = load_results_safely(dl_path, \"Deep Learning\")\n",
    "bert_results = load_results_safely(bert_path, \"Transformer\")\n",
    "\n",
    "all_results, all_results_sorted=compare_all_models(classical_results, dl_results, bert_results )\n",
    "\n",
    "# display(all_results_sorted)\n",
    "\n",
    "# Sauvegarder les résultats combinés\n",
    "combined_results_path = \"results/all_models_comparison.csv\"\n",
    "os.makedirs(os.path.dirname(combined_results_path), exist_ok=True)\n",
    "all_results.to_csv(combined_results_path, index=False)\n",
    "print(f\"Résultats combinés sauvegardés: {combined_results_path}\")\n",
    "\n",
    "# Afficher les résultats combinés\n",
    "print_md(\"\\n#### Résumé de tous les modèles:\")\n",
    "display(all_results[['Modèle', 'Type', 'F1 Score', 'Temps (s)', 'Taille (MB)']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbbf60-16ad-431a-af2d-3b7eb94e41bf",
   "metadata": {},
   "source": [
    "### 6.2 Trouver le meilleur modèle global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9829f6-67b1-4ca0-85f6-442f094ad5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Trouver le meilleur modèle global\n",
    "if not all_results.empty:\n",
    "    best_idx = all_results['F1 Score'].idxmax()\n",
    "    best_model = all_results.iloc[best_idx]\n",
    "\n",
    "    print_md(f\"\\n#### Meilleur modèle global: {best_model['Modèle']} ({best_model['Type']})\")\n",
    "    print_md(f\"F1 Score: **{best_model['F1 Score']:.4f}**\")\n",
    "    print_md(f\"Temps d'entraînement: **{best_model['Temps (s)']:.2f}** secondes\")\n",
    "    print_md(f\"Taille du modèle: **{best_model['Taille (MB)']:.2f}** MB\")\n",
    "else:\n",
    "    print_md(f\"### Aucun modèle disponible pour la comparaison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c8b14-bbc9-4a96-9533-5dbc747f3489",
   "metadata": {},
   "source": [
    "### 6.3 Visualisations comparatives globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12cf0e-d5c2-405e-bf81-02b0bbb24f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Visualisations comparatives globalesabs\n",
    "\n",
    "from utils import create_comparison_visualizations\n",
    "\n",
    "create_comparison_visualizations(all_results_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b8662-0246-47f4-99b7-f9fa7cbdc33f",
   "metadata": {},
   "source": [
    "### 6.4 Analyse du compromis performance/coût"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce74e56-0fad-4bcc-9d7e-a41ecbbadd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Analyse du compromis performance/coût\n",
    "\n",
    "from utils import calculate_global_score, visualize_global_score\n",
    "\n",
    "all_results_sorted = all_results.sort_values('Global_Score', ascending=False)\n",
    "print(\"\\nClassement des modèles selon le score global (compromis performance/vitesse/taille):\")\n",
    "display(all_results_sorted[['Modèle', 'Type', 'F1 Score', 'Temps (s)', 'Taille (MB)', 'Global_Score']])\n",
    "\n",
    "\n",
    "all_results_scored, all_results_sorted_ = calculate_global_score(all_results_sorted, perf_weight=0.6, speed_weight=0.2, compact_weight=0.2)\n",
    "\n",
    "visualize_global_score(all_results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1d7e7-0606-4da1-8f35-34b24fceac9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce04f49-c08d-4cc9-ab7d-40b0c86aaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Analyse du compromis performance/coût\n",
    "if not all_results.empty:\n",
    "    # Calculer un score de compromis\n",
    "    all_results['Performance'] = all_results['F1 Score']\n",
    "    all_results['Speed'] = 1 / np.log10(all_results['Temps (s)'] + 1)  # Inverse du log pour avoir un score plus élevé pour les modèles rapides\n",
    "    all_results['Compactness'] = 1 / np.log10(all_results['Taille (MB)'] + 1)  # Inverse du log pour avoir un score plus élevé pour les modèles compacts\n",
    "\n",
    "    # Normaliser les scores entre 0 et 1\n",
    "    for col in ['Performance', 'Speed', 'Compactness']:\n",
    "        min_val = all_results[col].min()\n",
    "        max_val = all_results[col].max()\n",
    "        if max_val > min_val:  # Éviter la division par zéro\n",
    "            all_results[col] = (all_results[col] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            all_results[col] = 0.5  # Valeur par défaut si tous les modèles ont la même valeur\n",
    "\n",
    "    # Calculer un score global (en donnant plus de poids à la performance)\n",
    "    all_results['Global_Score'] = (0.6 * all_results['Performance'] + \n",
    "                                  0.2 * all_results['Speed'] + \n",
    "                                  0.2 * all_results['Compactness'])\n",
    "\n",
    "    # Trier par score global\n",
    "    all_results_sorted = all_results.sort_values('Global_Score', ascending=False)\n",
    "    print(\"\\nClassement des modèles selon le score global (compromis performance/vitesse/taille):\")\n",
    "    display(all_results_sorted[['Modèle', 'Type', 'F1 Score', 'Temps (s)', 'Taille (MB)', 'Global_Score']])\n",
    "\n",
    "    # Visualiser le score global\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(\n",
    "            data=all_results_sorted,\n",
    "            x='Modèle',\n",
    "            y='Global_Score',\n",
    "            hue='Type',\n",
    "            palette='viridis'\n",
    "        )\n",
    "        plt.title('Score Global (60% Performance, 20% Vitesse, 20% Compacité)', fontsize=16)\n",
    "        plt.xlabel('Modèle', fontsize=14)\n",
    "        plt.ylabel('Score Global', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualisations/6_4_global_score_comparison.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la visualisation du score global: {e}\")\n",
    "else:\n",
    "    print(\"Pas assez de données pour l'analyse performance/coût\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fff050-55e0-4cea-a2e4-665038d47393",
   "metadata": {},
   "source": [
    "### 6.5 Recommandation finale pour Air Paradis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca0139-3a8a-42e0-b251-557b457cfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 Recommandation finale pour Air Paradis\n",
    "from utils import generate_model_recommendation\n",
    "\n",
    "print_md(\"\\n#### === Recommandation finale pour Air Paradis ===\")\n",
    "\n",
    "recommendation = generate_model_recommendation(all_results_sorted)\n",
    "print_md(recommendation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbc0f8-b656-4628-81c0-4dc2a658bc11",
   "metadata": {},
   "source": [
    "### 6.6 Stratégie de déploiement MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412168b2-8786-4b77-abbd-a5e464f01e23",
   "metadata": {},
   "source": [
    "#### Architecture de déploiement:\n",
    "- Modèle exposé via une API FastAPI\n",
    "- Déploiement sur Azure Web App (ou autre service Cloud)\n",
    "- Interface utilisateur en Streamlit pour tester l'API\n",
    "- Monitoring via Azure Application Insights\n",
    "\n",
    "#### Pipeline CI/CD:\n",
    "- Versionnement du code avec Git/GitHub\n",
    "- Tests automatisés à chaque commit\n",
    "- Déploiement automatique après tests réussis\n",
    "- Registre de modèles MLflow pour gérer les versions\n",
    "\n",
    "#### Suivi de performance:\n",
    "- Collecte des prédictions incorrectes signalées par les utilisateurs\n",
    "- Alertes en cas de détérioration des performances\n",
    "- Tableau de bord de suivi en temps réel\n",
    "- Réentraînement périodique avec de nouvelles données\n",
    "\n",
    "#### Amélioration continue:\n",
    "- Analyse régulière des tweets mal prédits\n",
    "- Enrichissement du dataset d'entraînement\n",
    "- Ajustement des hyperparamètres\n",
    "- Tests A/B pour valider les améliorations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce9d4b-522e-4589-ac71-6d1969abe871",
   "metadata": {},
   "source": [
    "### 6.7 Conclusion générale du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4d77d-93d4-4ef4-8b8a-afa4d5cc40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7 Conclusion générale du projet\n",
    "print_md(\"\\n#### === Conclusion générale du projet ===\")\n",
    "\n",
    "print(\"Dans ce projet, nous avons développé et comparé plusieurs approches pour l'analyse de sentiment des tweets:\")\n",
    "print(\"1. Modèles classiques (Régression logistique, Naive Bayes, SVM, Random Forest)\")\n",
    "print(\"2. Modèles deep learning (CNN, LSTM avec différentes stratégies d'embedding)\")\n",
    "print(\"3. Modèles Transformer (BERT, DistilBERT)\")\n",
    "\n",
    "print(\"\\nNous avons constaté que les modèles Transformer offrent les meilleures performances,\")\n",
    "print(\"mais au prix de ressources computationnelles plus importantes et d'une complexité accrue.\")\n",
    "print(\"Les modèles deep learning représentent un bon compromis, tandis que les modèles classiques\")\n",
    "print(\"sont plus rapides et plus légers, mais avec des performances inférieures.\")\n",
    "\n",
    "# Version qui fonctionne sans dépendre des cellules précédentes\n",
    "try:\n",
    "    # Essayer de lire les résultats finaux s'ils existent\n",
    "    combined_results_path = \"results/all_models_comparison.csv\"\n",
    "    if os.path.exists(combined_results_path):\n",
    "        all_results = pd.read_csv(combined_results_path)\n",
    "        if not all_results.empty:\n",
    "            best_model_idx = all_results['F1 Score'].idxmax()\n",
    "            recommended_model = all_results.iloc[best_model_idx]['Modèle']\n",
    "            print(f\"\\nLe modèle recommandé pour Air Paradis est {recommended_model},\")\n",
    "            print(\"qui offre le meilleur équilibre entre performance et contraintes de déploiement.\")\n",
    "        else:\n",
    "            raise ValueError(\"DataFrame vide\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Fichier de résultats non trouvé\")\n",
    "except Exception as e:\n",
    "    print(\"\\nSans résultats disponibles, nous recommandons généralement DistilBERT\")\n",
    "    print(\"comme meilleur compromis entre performance et contraintes de déploiement.\")\n",
    "    print(f\"(Remarque technique: {str(e)})\")\n",
    "\n",
    "print(\"\\nCe projet démontre l'importance d'une approche MLOps structurée pour:\")\n",
    "print(\"- Comparer rigoureusement différentes architectures\")\n",
    "print(\"- Suivre les expérimentations\")\n",
    "print(\"- Déployer et maintenir des modèles en production\")\n",
    "print(\"- Assurer une amélioration continue\")\n",
    "\n",
    "print(\"\\nCes méthodologies pourront être réutilisées pour d'autres projets d'analyse de sentiment,\")\n",
    "print(\"avec des adaptations aux spécificités des différents domaines d'application.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:air_paradis_p7] *",
   "language": "python",
   "name": "conda-env-air_paradis_p7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
